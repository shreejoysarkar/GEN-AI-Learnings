{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a562893",
   "metadata": {},
   "source": [
    "## Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263bd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech of Dr. APJ Abdul Kalam\n",
    "paragrpah = \"\"\"\n",
    "I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation.\n",
    "\n",
    "We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "\n",
    "We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.\n",
    "\n",
    "I have a THIRD VISION. India must stand up to the world. Because I believe that unless India stands up to the world, no one will respect us. Only strength respects strength. We must be strong not only as a military power but also as an economic power. Both must go hand-in-hand. My good fortune was to have worked with three great minds. Dr.Vikram Sarabhai, of the Dept. of Space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material. I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "\n",
    "I was in Hyderabad giving this lecture, when a 14 year-old girl asked me for my autograph. I asked her what her goal in life is. She replied: I want to live in a developed India. For her, you and I will have to build this developed India. You must proclaim India is not an underdeveloped nation; it is a highly developed nation.\n",
    "\n",
    "You say that our government is inefficient. You say that our laws are too old. You say that the municipality does not pick up the garbage. You say that the phones don’t work, the railways are a joke, the airline is the worst in the world, and mails never reach their destination. You say that our country has been fed to the dogs and is the absolute pits. You say, say and say. What do you do about it?\n",
    "\n",
    "Dear Indians, I am echoing J.F.Kennedy’s words to his fellow Americans to relate to Indians ……. “ASK WHAT WE CAN DO FOR INDIA AND DO WHAT HAS TO BE DONE TO MAKE INDIA WHAT AMERICA AND OTHER WESTERN COUNTRIES ARE TODAY.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c988c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11440e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20425b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb72fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a589d7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "sentences = nltk.sent_tokenize(paragrpah)  # para => sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a64b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply stopwords and filter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c35c4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom other .',\n",
       " 'that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'my second vision india develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top five nation world term gdp .',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr.vikram sarabhai , dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i hyderabad give lectur , 14 year-old girl ask autograph .',\n",
       " 'i ask goal life .',\n",
       " 'she repli : i want live develop india .',\n",
       " 'for , i build develop india .',\n",
       " 'you must proclaim india underdevelop nation ; highli develop nation .',\n",
       " 'you say govern ineffici .',\n",
       " 'you say law old .',\n",
       " 'you say municip pick garbag .',\n",
       " 'you say phone ’ work , railway joke , airlin worst world , mail never reach destin .',\n",
       " 'you say countri fed dog absolut pit .',\n",
       " 'you say , say say .',\n",
       " 'what ?',\n",
       " 'dear indian , i echo j.f.kennedi ’ word fellow american relat indian …… .',\n",
       " '“ ask what we can do for india and do what ha to be done to make india what america and other western countri are today . ”']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d50a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using snow  ball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer = SnowballStemmer('english')\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [SnowballStemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0090b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward greek , turk , mogul , portugu , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom .',\n",
       " 'first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'second vision india develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top five nation world term gdp .',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'on strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr.vikram sarabhai , dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'hyderabad give lectur , 14 year-old girl ask autograph .',\n",
       " 'ask goal life .',\n",
       " 'repli : want live develop india .',\n",
       " ', build develop india .',\n",
       " 'must proclaim india underdevelop nation ; high develop nation .',\n",
       " 'say govern ineffici .',\n",
       " 'say law old .',\n",
       " 'say municip pick garbag .',\n",
       " 'say phone ’ work , railway joke , airlin worst world , mail never reach destin .',\n",
       " 'say countri fed dog absolut pit .',\n",
       " 'say , say say .',\n",
       " '?',\n",
       " 'dear indian , echo j.f.kennedi ’ word fellow american relat indian …… .',\n",
       " '“ ask india ha done make india america western countri today . ”']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e4b77fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## using lemematization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i ]= sentences[i].lower()\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7a35a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori peopl world come invad u , captur land , conquer mind .',\n",
       " 'alexand onward greek , turk , mogul , portugu , british , french , dutch , came loot u , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom .',\n",
       " 'first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect u .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'second vision india develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top five nation world term gdp .',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect u .',\n",
       " 'strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr.vikram sarabhai , dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'hyderabad give lectur , 14 year-old girl ask autograph .',\n",
       " 'ask goal life .',\n",
       " 'repli : want live develop india .',\n",
       " ', build develop india .',\n",
       " 'must proclaim india underdevelop nation ; high develop nation .',\n",
       " 'say govern ineffici .',\n",
       " 'say law old .',\n",
       " 'say municip pick garbag .',\n",
       " 'say phone ’ work , railway joke , airlin worst world , mail never reach destin .',\n",
       " 'say countri fed dog absolut pit .',\n",
       " 'say , say say .',\n",
       " '?',\n",
       " 'dear indian , echo j.f.kennedi ’ word fellow american relat indian …… .',\n",
       " '“ ask india ha done make india america western countri today . ”']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d002a9ea",
   "metadata": {},
   "source": [
    "## Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4666c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech of Dr. APJ Abdul Kalam\n",
    "paragrpah = \"\"\"\n",
    "I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation.\n",
    "\n",
    "We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "\n",
    "We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.\n",
    "\n",
    "I have a THIRD VISION. India must stand up to the world. Because I believe that unless India stands up to the world, no one will respect us. Only strength respects strength. We must be strong not only as a military power but also as an economic power. Both must go hand-in-hand. My good fortune was to have worked with three great minds. Dr.Vikram Sarabhai, of the Dept. of Space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material. I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "\n",
    "I was in Hyderabad giving this lecture, when a 14 year-old girl asked me for my autograph. I asked her what her goal in life is. She replied: I want to live in a developed India. For her, you and I will have to build this developed India. You must proclaim India is not an underdeveloped nation; it is a highly developed nation.\n",
    "\n",
    "You say that our government is inefficient. You say that our laws are too old. You say that the municipality does not pick up the garbage. You say that the phones don’t work, the railways are a joke, the airline is the worst in the world, and mails never reach their destination. You say that our country has been fed to the dogs and is the absolute pits. You say, say and say. What do you do about it?\n",
    "\n",
    "Dear Indians, I am echoing J.F.Kennedy’s words to his fellow Americans to relate to Indians ……. “ASK WHAT WE CAN DO FOR INDIA AND DO WHAT HAS TO BE DONE TO MAKE INDIA WHAT AMERICA AND OTHER WESTERN COUNTRIES ARE TODAY.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f373f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\")\n",
    "sentences = nltk.sent_tokenize(paragrpah)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5c6bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nI have three visions for India.',\n",
       " 'In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture and their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.',\n",
       " 'That is why my FIRST VISION is that of FREEDOM.',\n",
       " 'I believe that India got its first vision of this in 1857, when we started the war of Independence.',\n",
       " 'It is this freedom that we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'We have 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isn’t this incorrect?',\n",
       " 'MY SECOND VISION for India is DEVELOPMENT.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among top five nations in the world in terms of GDP.',\n",
       " 'I have a THIRD VISION.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr.Vikram Sarabhai, of the Dept.',\n",
       " 'of Space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I was in Hyderabad giving this lecture, when a 14 year-old girl asked me for my autograph.',\n",
       " 'I asked her what her goal in life is.',\n",
       " 'She replied: I want to live in a developed India.',\n",
       " 'For her, you and I will have to build this developed India.',\n",
       " 'You must proclaim India is not an underdeveloped nation; it is a highly developed nation.',\n",
       " 'You say that our government is inefficient.',\n",
       " 'You say that our laws are too old.',\n",
       " 'You say that the municipality does not pick up the garbage.',\n",
       " 'You say that the phones don’t work, the railways are a joke, the airline is the worst in the world, and mails never reach their destination.',\n",
       " 'You say that our country has been fed to the dogs and is the absolute pits.',\n",
       " 'You say, say and say.',\n",
       " 'What do you do about it?',\n",
       " 'Dear Indians, I am echoing J.F.Kennedy’s words to his fellow Americans to relate to Indians …….',\n",
       " '“ASK WHAT WE CAN DO FOR INDIA AND DO WHAT HAS TO BE DONE TO MAKE INDIA WHAT AMERICA AND OTHER WESTERN COUNTRIES ARE TODAY.”']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34a0d479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "996ca892",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\shree/nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\shree\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m words = nltk.word_tokenize(sentence)\n\u001b[32m      6\u001b[39m words = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang, loc)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.save_dir = path_join(\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.TRAINED_TAGGER_PATH, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.TAGGER_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\tag\\perceptron.py:277\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang, loc)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m, loc=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loc:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_param\u001b[39m(json_file):\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_join(loc, json_file)) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\shree/nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\shree\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    print(nltk.pos_tag(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710ce13",
   "metadata": {},
   "source": [
    "## Named Entity recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa71d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The Eiffel Tower is one of the most famous landmarks in Paris.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf45a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10561ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Eiffel',\n",
       " 'Tower',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'famous',\n",
       " 'landmarks',\n",
       " 'in',\n",
       " 'Paris',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb7e67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6d144b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\shree/nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\shree\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m nltk.ne_chunk(\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang, loc)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.save_dir = path_join(\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.TRAINED_TAGGER_PATH, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.TAGGER_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\tag\\perceptron.py:277\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang, loc)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m, loc=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loc:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_param\u001b[39m(json_file):\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_join(loc, json_file)) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shree\\anaconda3\\envs\\genai\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\shree/nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\shree\\\\anaconda3\\\\envs\\\\genai\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\shree\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.ne_chunk(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d22840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
